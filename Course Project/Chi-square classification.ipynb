{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Import all necessary library files'''\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import operator\n",
    "import collections\n",
    "from math import log\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Praveen\\Desktop\\CS-579-Project\\Senti-Analysis\n"
     ]
    }
   ],
   "source": [
    "cd Senti-Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data split into positive and negative:**\n",
    "\n",
    "We split our dataset by considering the reviews with scores above 65 to be Positive and those ones that are below 65 to be considered as negative. Thus we create a new column called as 'score' and then we populate it by using binary values according to the nature of its Sentiment score given by the critic. Thus a review with Sentiment score greater than '65' would have a score of '1' and all others would have '0'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>﻿Review</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That such intelligence could be contained in a...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shines with a kind of inspired madness.</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Here is the most passionate and tender love st...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the greatest of all American films, but...</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A remarkable film.</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ﻿Review  Sentiment  score\n",
       "0  That such intelligence could be contained in a...        100      1\n",
       "1            Shines with a kind of inspired madness.        100      1\n",
       "2  Here is the most passionate and tender love st...        100      1\n",
       "3  One of the greatest of all American films, but...        100      1\n",
       "4                                 A remarkable film.        100      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Place the reviews.txt file in the working directory'''\n",
    "df = pd.read_csv(\"reviews.txt\",sep='\\t')\n",
    "df['score'] = [1 if x > 65 else 0 for x in df['Sentiment']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Create two separate dataframes based on Sentiment scores'''\n",
    "pos = df[df['Sentiment'] > 65]\n",
    "neg = df[df['Sentiment'] < 65]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21027\n",
      "22843\n"
     ]
    }
   ],
   "source": [
    "pos = pos.drop('Sentiment', 1)\n",
    "neg = neg.drop('Sentiment',1)\n",
    "print len(pos)\n",
    "print len(neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11027\n",
      "12843\n"
     ]
    }
   ],
   "source": [
    "'''Dividing the entire dataset into new training and testing dataframes'''\n",
    "pos_train = pos.head(10000)\n",
    "neg_train = neg.head(10000)\n",
    "pos_test = pos.head(len(pos)-len(pos_train))\n",
    "neg_test = neg.head(len(neg)-len(neg_train))\n",
    "print len(pos_test)\n",
    "print len(neg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>﻿Review</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That such intelligence could be contained in a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shines with a kind of inspired madness.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Here is the most passionate and tender love st...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the greatest of all American films, but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A remarkable film.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             ﻿Review  score\n",
       "0  That such intelligence could be contained in a...      1\n",
       "1            Shines with a kind of inspired madness.      1\n",
       "2  Here is the most passionate and tender love st...      1\n",
       "3  One of the greatest of all American films, but...      1\n",
       "4                                 A remarkable film.      1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>﻿Review</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1412</th>\n",
       "      <td>Exhibiting high spirits and a crazed comic ene...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>There is something repulsive and manipulative ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1414</th>\n",
       "      <td>The part that needs work didn't cost money. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1415</th>\n",
       "      <td>The movie does have charm and moments of humor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1416</th>\n",
       "      <td>Sandler, at the center, is a distraction; he s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                ﻿Review  score\n",
       "1412  Exhibiting high spirits and a crazed comic ene...      0\n",
       "1413  There is something repulsive and manipulative ...      0\n",
       "1414  The part that needs work didn't cost money. It...      0\n",
       "1415  The movie does have charm and moments of humor...      0\n",
       "1416  Sandler, at the center, is a distraction; he s...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Performing Concatenation of both positive and negative training dataframes'''\n",
    "frames = [pos_train,neg_train]\n",
    "result = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "'''Printing the length of training dataframe and then re-ordering the dataframe columns'''\n",
    "print len(result)\n",
    "result = result.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Write the data to a temporary csv file\n",
    "result.to_csv(\"test-ex.csv\", sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11027\n",
      "12843\n"
     ]
    }
   ],
   "source": [
    "'''Total positive and negative test dataframes'''\n",
    "print len(pos_test)\n",
    "print len(neg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Concatenation of both positive and negative test dataframes together into a single dataframe \n",
    "    and writing it to a csv file.'''\n",
    "\n",
    "frames2 = [pos_test,neg_test]\n",
    "result2 = pd.concat(frames2)\n",
    "result2 = result2.drop('score',1)\n",
    "result2.to_csv(\"test-ex2.csv\",sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Removing unwanted columns and storing the data into a new csv file'''\n",
    "\n",
    "with open(\"test-ex.csv\",\"rb\") as source:\n",
    "    rdr= csv.reader( source )\n",
    "    with open(\"train.csv\",\"wb\") as result:\n",
    "        wtr= csv.writer( result )\n",
    "        for r in rdr:\n",
    "            wtr.writerow( (r[1], r[2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Modules to perform text pre-processing'''\n",
    "# Remove Punctuations\n",
    "def removePunctuation(input_list):\n",
    "    punctuation=re.compile(r'[,./?!\":;|-]')\n",
    "    punct_remove = [punctuation.sub(\" \", word) for word in input_list]\n",
    "    return punct_remove\n",
    "\n",
    "# Remove Stop Words\n",
    "def removeStopWords(input_list):\n",
    "    stop=stopwords.words('english')\n",
    "    #We try to preserve not,nor and no as these may prove to be influential during classification.\n",
    "    stop.remove('not')\n",
    "    stop.remove('nor')\n",
    "    stop.remove('no')\n",
    "    stop.append('')\n",
    "    \n",
    "    for s in stop:\n",
    "        while s in input_list:\n",
    "            input_list.remove(s)\n",
    "    return input_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training our classification model:**\n",
    "\n",
    "We read each line of our training data stored under the name of train.csv file and we carry out the initial tokenization functionalities. We then perform feature extraction procedures to fill out the positive and negative feature dictionaries, by reading each review with reference to the actual true label score of that particular review. To be precise, when we read a review whose true label score is 1 we then try to insert those tokenized words into the positive feature dictionary along with their corresponding frequency of word occurrence. Likewise we tend to populate our negative feature dictionary if the actual true label of a given review is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def classify_train(filename):\n",
    "    \n",
    "    pos_dic={}\n",
    "    neg_dic={}\n",
    "    tot_dic={}\n",
    "    pos_review_count=0\n",
    "    neg_review_count=0\n",
    "    total=0\n",
    "    flag=False\n",
    "    \n",
    "\n",
    "    #Reading the Reviews from Training Data and Building Feature Dictionary\n",
    "\n",
    "    with open (filename,'rb') as training_file:\n",
    "        training_data=csv.reader(training_file)\n",
    "\n",
    "        for row in training_data:\n",
    "            #Using this flag to skip the first row in the csv\n",
    "            if flag==False:\n",
    "                flag=True\n",
    "            else:\n",
    "                total+=1\n",
    "                word_list=re.split('\\s+',row[1].lower())           \n",
    "                word_list_punct=removePunctuation(word_list)     \n",
    "                word_list_stop=removeStopWords(word_list_punct)\n",
    "                words_count = collections.Counter(word_list_stop)\n",
    "                if int(row[0])==1:\n",
    "                    pos_review_count+=1\n",
    "                    for word in words_count:   \n",
    "                        if word in tot_dic:\n",
    "                            f_value = tot_dic[word]\n",
    "                            f_value[0] += words_count[word]\n",
    "                            f_value[1] += 1\n",
    "                            tot_dic[word] = f_value\n",
    "                        else:\n",
    "                            tot_dic[word] = [words_count[word], 1]\n",
    "                        if word in pos_dic:\n",
    "                            f_value = pos_dic[word]\n",
    "                            f_value[0] += words_count[word]\n",
    "                            f_value[1] += 1\n",
    "                            pos_dic[word] = f_value\n",
    "                        else:\n",
    "                            pos_dic[word]= [words_count[word], 1]\n",
    "\n",
    "                else: \n",
    "                    neg_review_count+=1\n",
    "                    for word in words_count:\n",
    "                        word=word\n",
    "                        if word in tot_dic:\n",
    "                            f_value = tot_dic[word]\n",
    "                            f_value[0] += words_count[word]\n",
    "                            f_value[1] += 1\n",
    "                            tot_dic[word] = f_value\n",
    "                        else:\n",
    "                            tot_dic[word] = [words_count[word], 1]    \n",
    "                        if word in neg_dic:\n",
    "                            f_value = neg_dic[word]\n",
    "                            f_value[0] += words_count[word]\n",
    "                            f_value[1] += 1                    \n",
    "                            neg_dic[word] = f_value\n",
    "                        else:\n",
    "                            neg_dic[word]= [words_count[word], 1]\n",
    "    training_file.close()\n",
    "    \n",
    "    return pos_dic,neg_dic,tot_dic,total,pos_review_count,neg_review_count\n",
    "\n",
    "#pos_dic,neg_dic,tot_dic,total,pos_review_count,neg_review_count = classify_train('train.csv')                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Positive and negative feature dictionaries:**\n",
    "\n",
    "The positive and negative dictionary represents a dictionary of key value pairs where the dictionary key is a positive/negative word and the corresponding value is a list where the first element of the list indicates the total occurrence of that particular word in the entire training set and the second element of the list indicates the total number of documents in which the particular word occurs atleast once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'  for': [1, 1], ' ': [465, 315], '  ': [652, 450], '   ': [9, 7], ' 21': [2, 2], '  and': [2, 2], '  transcendentally': [1, 1], '  you': [1, 1], ' 11  ': [1, 1], '  is': [1, 1], ' 2001 ': [5, 5], ' 60': [1, 1], '  especially': [1, 1], ' 3rd': [1, 1], '  to': [1, 1], ' 13': [1, 1], \" 2001 ''\": [1, 1], ' 300 ': [2, 2], '  smart': [1, 1], ' &#8211 ': [4, 2]}\n",
      "{' (uninteresting)  ': [1, 1], ' ': [407, 283], '  ': [637, 436], '   ': [10, 10], ' 21': [1, 1], ' 24 ': [1, 1], '  and': [1, 1], ' 2001  ': [1, 1], '  the': [1, 1], ' (unwatched) ': [1, 1], \" 'home\": [1, 1], ' &#150 ': [4, 4], ' 28': [1, 1], '  only': [1, 1], ' &#8211 ': [1, 1], ' 1941  ': [1, 1], ' (500)': [1, 1], ' 24  ': [1, 1], ' 27': [1, 1], '  ouch ': [1, 1]}\n"
     ]
    }
   ],
   "source": [
    "'''Inorder to execute this below code run the above module by removing the commented line in the last and then run this.'''\n",
    "firstfewpairs_pos = {k: pos_dic[k] for k in sorted(pos_dic.keys())[:20]}\n",
    "print firstfewpairs_pos\n",
    "firstfewpairs_neg = {k: neg_dic[k] for k in sorted(neg_dic.keys())[:20]}\n",
    "print firstfewpairs_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature selection method - Chi-square**\n",
    "\n",
    "The x2 test is used in statistics, among other things, to test the independence of two events. More specifically in feature selection we use it to test whether the occurrence of a specific term and the occurrence of a specific class are independent. Thus we estimate the following quantity for each term and we rank them by their score: Below is the formal representation of the chi-square feature selection methodology. We decided to implement the same ideology to our text classification problem and experimented by choosing certain parameter features and hence tested on its prediction accuracy. \n",
    "<img src=\"chi-square formula.png\"width=700>\n",
    "We are also aware of the fact that high scores on x2 indicate that the null hypothesis (H0) of independence should be rejected and thus that the occurrence of the term and class are dependent. If they are dependent then we select the feature for the text classification.\n",
    "\n",
    "Although from a statistical point of view, using the chi-square feature selection might be problematic as whenever we tend to use this statistical test multiple times, then its said that the probability of getting at least one error increases. However, in text classification it rarely matters whether a few additional terms are added to the feature set or removed from it. Rather, the relative importance of features is important. As long as x2 feature selection only ranks features with respect to their usefulness and is not used to make statements about statistical dependence or independence of variables, we need not be overly concerned that it does not adhere strictly to statistical theory.\n",
    "\n",
    "Below are the stats to be estimated inorder to apply them in the formula,\n",
    "\n",
    "-n11 - The total number of reviews where the word occurs and provided the review was labelled positive.\n",
    "\n",
    "-n10 - The total number of reviews where the word occurs and the review was labelled negative.\n",
    "\n",
    "-n01 - The total number of reviews where the word does not appear and the review was labelled positive.\n",
    "\n",
    "-n00 - The total number of reviews where the word does not appear and the review was labelled to be negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Feature Selection, Computation of Chi-Squared test statistic, we finally sort the dictionary to obtain the top elements. \n",
    " We will use this module under evaluation function.'''\n",
    "\n",
    "'''Do not execute this block of code now. This chunk is written here only to show how we calculate the above formula'''\n",
    "\n",
    "chi_sq={}\n",
    "for feature in tot_dic:\n",
    "    if feature in pos_dic:        \n",
    "        n11=pos_dic[feature][1]\n",
    "    else:\n",
    "        n11=0\n",
    "    if feature in neg_dic:\n",
    "        n10=neg_dic[feature][1]\n",
    "    else:\n",
    "        n10=0\n",
    "    n01=p_count-n11\n",
    "    n00=n_count-n10\n",
    "\n",
    "    chi_sq[feature]=((float) (((n11+n10+n01+n00)*pow((n11*n00-n10*n01),2))/(float)((n11+n01)*(n11+n10)*(n10+n00*n01+n00))))\n",
    "\n",
    "#Sort Vocab dictionary in descending order of Chi-Squared Statistic\n",
    "sorted_chi_sq=sorted(chi_sq.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
    "pos_prior=(float)(p_count)/(total)\n",
    "neg_prior=(float)(n_count)/(total)\n",
    "\n",
    "total_vocab=len(tot_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature tuning**\n",
    "\n",
    "In the below function, we check both the positive and negative feature dictionaries and thereby remove unimportant features present in both dictionaries. Based on the value of k, we pick out top k values from the sorted chi-squared dictionary and name them to be important features to be selected, after which we compare these important features with all the keys present in both the positive and negative dictionaries and hence filter them out. Finally we output the dictionaries using pickle for later usage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Set the value of k to tune the number of features selected\n",
    "def tune_features(k,sorted_chi_sq,pos_dic,neg_dic,tot_dic):\n",
    "    #k=6000\n",
    "\n",
    "    # Delete the unimportant features from the positive and negative dictionaries\n",
    "    imp_feature_list=[]\n",
    "\n",
    "    for [key,val] in sorted_chi_sq:\n",
    "        imp_feature_list.append(key)\n",
    "        if(len(imp_feature_list)==k):\n",
    "            break\n",
    "    for key in pos_dic.keys():\n",
    "        if key not in imp_feature_list:\n",
    "            pos_dic.pop(key)\n",
    "            if key in tot_dic:\n",
    "                tot_dic.pop(key)\n",
    "    for key in neg_dic.keys():\n",
    "        if key not in imp_feature_list:\n",
    "            neg_dic.pop(key)\n",
    "            if key in tot_dic:\n",
    "                tot_dic.pop(key)\n",
    "    f=open('pos_dictionary.txt','w')\n",
    "    pickle.dump(pos_dic,f)\n",
    "    f.close()\n",
    "\n",
    "    f=open('neg_dictionary.txt','w')\n",
    "    pickle.dump(neg_dic,f)\n",
    "    f.close()\n",
    "\n",
    "    f=open('vocabulary.txt','w')\n",
    "    pickle.dump(tot_dic,f)\n",
    "    f.close()\n",
    "    print 'Lengths of pos,neg dictionaries for k=',k,'after feature extraction',len(pos_dic),len(neg_dic)\n",
    "    return pos_dic,neg_dic,tot_dic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes Classification on testing data using Chi-square feature selection strategy**\n",
    "\n",
    "The Naive Bayesian classifier is based on Bayes’ theorem with independence assumptions between predictors. A Naive Bayesian model is easy to build, with no complicated iterative parameter estimation which makes it particularly useful for very large datasets. Despite its simplicity, the Naive Bayesian classifier often does surprisingly well and is widely used because it often outperforms more sophisticated classification methods. \n",
    "\n",
    "Bayes theorem provides a way of calculating the posterior probability, P(c|x), from P(c), P(x), and P(x|c). Naive Bayes classifier assume that the effect of the value of a predictor (x) on a given class (c) is independent of the values of other predictors. This assumption is called class conditional independence.\n",
    "\n",
    "A formal representation of the naive bayes algorithm is below,\n",
    "\n",
    "<img src=\"nb.png\"width=800px>\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Testing our classification model using naive bayes approach'''\n",
    "\n",
    "def classify_test(filename,pos_prior,neg_prior):\n",
    "    #Variable Declarations\n",
    "    flag=False\n",
    "    testing_example_count=0\n",
    "    token_list=[]\n",
    "    output_list=[]\n",
    "    total_test_pos_reviews=0\n",
    "    total_test_neg_reviews=0\n",
    "    totalfreqpos=0\n",
    "    totalfreqneg = 0\n",
    "\n",
    "    #Import the trained model files\n",
    "    f=open('pos_dictionary.txt','r')\n",
    "    pos_dic=pickle.load(f)\n",
    "    f=open('neg_dictionary.txt','r')\n",
    "    neg_dic=pickle.load(f)\n",
    "\n",
    "    f=open('vocabulary.txt','r')\n",
    "    tot_dic=pickle.load(f)\n",
    "\n",
    "    #Classification \n",
    "    with open (filename,'rb') as testing_file:\n",
    "        testing_data=csv.reader(testing_file)\n",
    "        for row in testing_data:\n",
    "            token_pos_prob_list=[]\n",
    "            token_neg_prob_list=[]\n",
    "            outputList=[]\n",
    "            if flag==False:\n",
    "                flag=True\n",
    "                i=0\n",
    "            else:\n",
    "                testing_example_count+=1\n",
    "                word_list_split=re.split('\\s+',row[1].lower())\n",
    "                word_list_minus_punct=removePunctuation(word_list_split)\n",
    "                word_list_minus_stop=removeStopWords(word_list_minus_punct)\n",
    "                for word in word_list_minus_stop:\n",
    "                    token_list.append(word)\n",
    "                    if word in pos_dic:\n",
    "                        pos_prob_token=(float)(pos_dic[word][0]+1)/(len(tot_dic)+totalfreqpos)\n",
    "                        token_pos_prob_list.append(math.log(pos_prob_token))\n",
    "                    else:\n",
    "                        token_pos_prob_list.append(math.log((float)(1)/(len(tot_dic)+totalfreqpos)))\n",
    "                    if word in neg_dic:\n",
    "                        neg_prob_token=(float)(neg_dic[word][0]+1)/(len(tot_dic)+totalfreqneg)\n",
    "                        token_neg_prob_list.append(math.log(neg_prob_token))\n",
    "                    else:\n",
    "                        token_neg_prob_list.append(math.log((float)(1)/(len(tot_dic)+totalfreqneg)))\n",
    "\n",
    "                pos_prob_total=0\n",
    "                pos_prob_total = reduce(lambda a,d: a + d,token_pos_prob_list)\n",
    "                pos_prob_total = pos_prob_total + math.log(pos_prior)\n",
    "                neg_prob_total = math.log(neg_prior) + (reduce(lambda a, d:a + d,token_neg_prob_list))\n",
    "                if pos_prob_total>neg_prob_total:\n",
    "                    review=1\n",
    "                    total_test_pos_reviews+=1\n",
    "                else:\n",
    "                    review=0\n",
    "                    total_test_neg_reviews+=1\n",
    "                i+=1\n",
    "                output_list.append([i, review])\n",
    "        c = csv.writer(open(\"result.csv\", \"wb\"))\n",
    "        c.writerow(['Review-ID', 'Predicted score'])\n",
    "        for row in output_list:\n",
    "            c.writerow(row)\n",
    "\n",
    "        testing_file.close()\n",
    "        return total_test_pos_reviews,total_test_neg_reviews,testing_example_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Creating a new csv file containing the true label score and reviews of testing dataframe'''\n",
    "frames2 = [pos_test,neg_test]\n",
    "result2 = pd.concat(frames2)\n",
    "result2= result2.sort_index(axis=1)\n",
    "result2.to_csv(\"test-accuracy.csv\",sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Remove unwanted columns and write data to a new csv file which will be used in the evaluation module to compute accuracy'''\n",
    "with open(\"test-accuracy.csv\",\"rb\") as source:\n",
    "    rdr= csv.reader( source )\n",
    "    with open(\"test-data.csv\",\"wb\") as result:\n",
    "        wtr= csv.writer( result )\n",
    "        for r in rdr:\n",
    "            wtr.writerow( (r[1], r[2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Get the true labels for all reviews in the testing data csv file and store them into a python list. This list will be used \n",
    " to compare and hence calculate classification accuracy'''\n",
    "test_score=[]\n",
    "with open(\"test-data.csv\",\"rb\") as result:\n",
    "    rdr= csv.reader( result ) \n",
    "    for r in rdr:\n",
    "        test_score.append(r[0])\n",
    "test_score.remove('score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this evaluation function by passing a list of parameter values that represent the total number of chi-squared \n",
    "features to be considered while trying to classify testing data. Examine the accuracy variation for each parameter change\n",
    "and hence choose the best tuning parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Run this evaluation function by passing a list of parameter values that represent the total number of chi-squared \n",
    "features to be considered while trying to classify testing data. Examine the accuracy variation for each parameter change\n",
    "and hence choose the best tuning parameter. '''\n",
    "\n",
    "def evaluation(param):\n",
    "    for k in param:\n",
    "        pos_dic,neg_dic,tot_dic,total,p_count,n_count = classify_train(\"train.csv\")\n",
    "        chi_sq={}\n",
    "        for feature in tot_dic:\n",
    "            if feature in pos_dic:        \n",
    "                n11=pos_dic[feature][1]\n",
    "            else:\n",
    "                n11=0\n",
    "            if feature in neg_dic:\n",
    "                n10=neg_dic[feature][1]\n",
    "            else:\n",
    "                n10=0\n",
    "            n01=p_count-n11\n",
    "            n00=n_count-n10\n",
    "\n",
    "            chi_sq[feature]=((float) (((n11+n10+n01+n00)*pow((n11*n00-n10*n01),2))/(float)((n11+n01)*(n11+n10)*(n10+n00*n01+n00))))\n",
    "\n",
    "        #Sort Vocab dictionary in descending order of Chi-Squared Statistic\n",
    "        sorted_chi_sq=sorted(chi_sq.iteritems(),key=operator.itemgetter(1),reverse=True)\n",
    "        pos_prior=(float)(p_count)/(total)\n",
    "        neg_prior=(float)(n_count)/(total)\n",
    "\n",
    "        total_vocab=len(tot_dic)\n",
    "    \n",
    "        pos_dic2,neg_dic2,tot_dic2 = tune_features(k,sorted_chi_sq,pos_dic,neg_dic,tot_dic)\n",
    "        print'Some statistics for a given k=',k\n",
    "        print 'The vocabulary contains',total_vocab,'words'\n",
    "        print 'Positive Features Dictionary',len(pos_dic2)\n",
    "        print 'Negative Features Dictionary',len(neg_dic2)\n",
    "        print 'Chi-Squared Values',len(tot_dic2)\n",
    "        print'PRIORS',pos_prior,neg_prior\n",
    "        print '----------------------------------------------------------'\n",
    "        print 'Now lets see how our classification model performs'\n",
    "        total_p_reviews2,total_n_reviews2,t_count2 = classify_test('test-ex2.csv',pos_prior,neg_prior)\n",
    "        print 'Total Positives: ',total_p_reviews2,'Total Negatives: ',total_n_reviews2\n",
    "        predicted_score=[]\n",
    "        with open(\"result.csv\",\"rb\") as result:\n",
    "            rdr= csv.reader( result ) \n",
    "            for r in rdr:\n",
    "                predicted_score.append(r[1])\n",
    "        predicted_score.remove('Predicted score')\n",
    "        final = [i for i, j in zip(test_score, predicted_score) if i == j]\n",
    "        print 'We get %.4g classification accuracy' % ((len(final)/len(test_score)) * 100)\n",
    "        print'End -------------------------of--------------------------------statistics'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Praveen\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:18: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "C:\\Users\\Praveen\\Anaconda\\lib\\site-packages\\IPython\\kernel\\__main__.py:19: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of pos,neg dictionaries for k= 5000 after feature extraction 3737 3885\n",
      "Some statistics for a given k= 5000\n",
      "The vocabulary contains 47991 words\n",
      "Positive Features Dictionary 3737\n",
      "Negative Features Dictionary 3885\n",
      "Chi-Squared Values 5000\n",
      "PRIORS 0.5 0.5\n",
      "----------------------------------------------------------\n",
      "Now lets see how our classification model performs\n",
      "Total Positives:  11905 Total Negatives:  11965\n",
      "We get 83.78 classification accuracy\n",
      "End -------------------------of--------------------------------statistics\n",
      "Lengths of pos,neg dictionaries for k= 6000 after feature extraction 4431 4574\n",
      "Some statistics for a given k= 6000\n",
      "The vocabulary contains 47991 words\n",
      "Positive Features Dictionary 4431\n",
      "Negative Features Dictionary 4574\n",
      "Chi-Squared Values 6000\n",
      "PRIORS 0.5 0.5\n",
      "----------------------------------------------------------\n",
      "Now lets see how our classification model performs\n",
      "Total Positives:  11889 Total Negatives:  11981\n",
      "We get 84.55 classification accuracy\n",
      "End -------------------------of--------------------------------statistics\n",
      "Lengths of pos,neg dictionaries for k= 7000 after feature extraction 4966 5039\n",
      "Some statistics for a given k= 7000\n",
      "The vocabulary contains 47991 words\n",
      "Positive Features Dictionary 4966\n",
      "Negative Features Dictionary 5039\n",
      "Chi-Squared Values 7000\n",
      "PRIORS 0.5 0.5\n",
      "----------------------------------------------------------\n",
      "Now lets see how our classification model performs\n",
      "Total Positives:  11887 Total Negatives:  11983\n",
      "We get 85.15 classification accuracy\n",
      "End -------------------------of--------------------------------statistics\n",
      "Lengths of pos,neg dictionaries for k= 8000 after feature extraction 5443 5562\n",
      "Some statistics for a given k= 8000\n",
      "The vocabulary contains 47991 words\n",
      "Positive Features Dictionary 5443\n",
      "Negative Features Dictionary 5562\n",
      "Chi-Squared Values 8000\n",
      "PRIORS 0.5 0.5\n",
      "----------------------------------------------------------\n",
      "Now lets see how our classification model performs\n",
      "Total Positives:  11865 Total Negatives:  12005\n",
      "We get 85.72 classification accuracy\n",
      "End -------------------------of--------------------------------statistics\n",
      "Lengths of pos,neg dictionaries for k= 9000 after feature extraction 5874 6131\n",
      "Some statistics for a given k= 9000\n",
      "The vocabulary contains 47991 words\n",
      "Positive Features Dictionary 5874\n",
      "Negative Features Dictionary 6131\n",
      "Chi-Squared Values 9000\n",
      "PRIORS 0.5 0.5\n",
      "----------------------------------------------------------\n",
      "Now lets see how our classification model performs\n",
      "Total Positives:  11832 Total Negatives:  12038\n",
      "We get 86.22 classification accuracy\n",
      "End -------------------------of--------------------------------statistics\n"
     ]
    }
   ],
   "source": [
    "'''Call evaluation function by passing a list of parameter values. '''\n",
    "\n",
    "evaluation([5000,6000,7000,8000,9000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Inference**\n",
    "\n",
    "By looking at the performance of our classifier, we could say that the top accuracy has been obtained for k=9000. Even otherwise the accuracy seems to increase on a linear fashion as we increase the total number of features to include. This approach of using chi-squared feature selection method is also proved to have positive impact during classification as the results are much better when compared to our classification performance carried out by Logistic regression and the multinomial naive bayes models. However, In our case since we have about 47991 words present in the vocabulary set, we could select the number of influential features to be anywhere between 8000 and 10000 to thereby achieve optimal classification. Increasing the tuning parameter even more might result in biased performance. Overall we would like to say that, given the negativity involved in using the chi-square test statistic measure as a feature selection strategy, the naive bayes classifier performs well to meet our expectation. But still while choosing the number of features 'k', we would recommend choosing 'k' by considering the length of the vocabulary and the size of the training set. Choosing more number of 'k' features just to attain better accuracy may lead to extreme bias and memory constraints.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

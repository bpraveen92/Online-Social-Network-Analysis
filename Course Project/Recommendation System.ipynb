{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Import necessary library'''\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User-User Collaborative filtering approach**\n",
    "\n",
    "User–user CF is a straightforward algorithmic interpretation of the core premise of collaborative filtering: find other users whose past rating behavior is similar to that of the current user and use their ratings on other items to predict what the current user will like. Lets say for example, To predict Mary’s preference for an item she has not rated, user–user CF looks\n",
    "for other users who have high agreement with Mary on the items they have both rated. These users’ ratings for the item in question are then weighted by their level of agreement with Mary’s ratings to predict Mary’s preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('dict_scores_for_25_critics.txt', 'rb') as handle:\n",
    "    dataset = pickle.loads(handle.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity Function**\n",
    "\n",
    "A critical design decision in implementing user–user CF is the choice of similarity function. We will try using two functions to estimate the similarity between two users or critics. One is the euclidean distance metric that calculates the similarity between two users and the other one turns out to be the more efficient pearson co-efficient. The problem with euclidean distance is that it measures the dissimilarity too, In our case, the people who like the same movies are less important than the ones that prefer different movies or that they have different tastes. To be more precise, instead of just relying on the most similar person, a prediction is normally based on the weighted average of the recommendations of several people. The weight given to a person’s ratings is determined by the correlation between that person and the person for whom to make a prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def similarity_score(critic1,critic2):\n",
    "    \n",
    "    '''Returns ratio Euclidean distance score of critic1 and critic2''' \n",
    "\n",
    "    common = {} # To get both rated movies by critic1 and critic2\n",
    "\n",
    "    for movie in dataset[critic1]:\n",
    "        if movie in dataset[critic2]:\n",
    "            common[movie] = 1\n",
    "\n",
    "        # Conditions to check they both have an common rating movies\n",
    "        if len(common) == 0:\n",
    "            return 0\n",
    "\n",
    "        # Finding Euclidean distance \n",
    "        ed = []\n",
    "\n",
    "        for movie in dataset[critic1]:\n",
    "            if movie in dataset[critic2]:\n",
    "                ed.append(pow(dataset[critic1][movie] - dataset[critic2][movie],2))\n",
    "        ed = sum(ed)\n",
    "\n",
    "        return 1/(1+sqrt(ed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pearson co-efficient**\n",
    "\n",
    "This method computes the statistical correlation (Pearson’s r) between two user’s common ratings to determine their similarity. This measures how well two critics are linearly related in our case. A formal representation of the pearson-co-efficient is given below,\n",
    "<img src=\"pcc.png\"width=700px> \n",
    "\n",
    "The below table gives additional details about each and every variable used in the above formula,\n",
    "\n",
    "<img src=\"pcc_2.png\"width=700px> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pearson_correlation(critic1,critic2):\n",
    "\n",
    "# Fetch the commonly rated items\n",
    "    common = {}\n",
    "    for item in dataset[critic1]:\n",
    "        if item in dataset[critic2]:\n",
    "            common[item] = 1\n",
    "\n",
    "    total_ratings = len(common)\n",
    "    \n",
    "    if total_ratings == 0:\n",
    "        return 0\n",
    "\n",
    "    # Add all the ratings given by both the critics for all the movies that are in common between them.\n",
    "    critic1_pref_sum = sum([dataset[critic1][item] for item in common])\n",
    "    critic2_pref_sum = sum([dataset[critic2][item] for item in common])\n",
    "\n",
    "    # Sum up the squares of ratings of each user.\n",
    "    critic1_ratings_sum = sum([pow(dataset[critic1][item],2) for item in common])\n",
    "    critic2_ratings_sum = sum([pow(dataset[critic2][item],2) for item in common])\n",
    "\n",
    "    # Sum up the product value of both preferences for each item\n",
    "    product_sum_of_both_users = sum([dataset[critic1][item] * dataset[critic2][item] for item in common])\n",
    "\n",
    "    # Calculate the pearson score\n",
    "    n1 = product_sum_of_both_users - (critic1_pref_sum*critic2_pref_sum/total_ratings)\n",
    "    d1 = sqrt((critic1_ratings_sum - pow(critic1_pref_sum,2)/total_ratings) * \n",
    "                             (critic2_ratings_sum -pow(critic2_pref_sum,2)/total_ratings))\n",
    "    if d1 == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        rating = n1/d1\n",
    "        return rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reference**\n",
    "\n",
    "Implemented these functionalities by gaining knowledge from the book called Programming collective Intelligence written by Toby Segaran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_similar_users(critic,n):\n",
    "    '''returns the number_of_users (similar critics) for a given specific critic by estimating the \n",
    "    pearson correlation score between the given critic and all other critics in the dataset.'''\n",
    "    \n",
    "    scores = [(pearson_correlation(critic,other_critic),other_critic) for other_critic in dataset if  other_critic != critic ]\n",
    "    scores.sort()\n",
    "    scores.reverse()\n",
    "    return scores[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_reommendations(critic,n):\n",
    "\n",
    "    '''Gets recommendations for a critic by using a weighted average of every other user's rankings'''\n",
    "    totals = {}\n",
    "    simSums = {}\n",
    "    for other in dataset:\n",
    "        if other != critic:\n",
    "            sim = pearson_correlation(critic,other)\n",
    "            if sim <=0: \n",
    "                continue\n",
    "            for item in dataset[other]:\n",
    "                # only score movies i haven't seen yet\n",
    "                if item not in dataset[critic] or dataset[critic][item] == 0:\n",
    "                # Similrity * score\n",
    "                    totals.setdefault(item,0)\n",
    "                    totals[item] = totals[item] + dataset[other][item]* sim\n",
    "                    # sum of similarities\n",
    "                    simSums.setdefault(item,0)\n",
    "                    simSums[item]+= sim\n",
    "\n",
    "    rankings = [(total/simSums[item],item) for item,total in totals.items()]\n",
    "    rankings.sort()\n",
    "    rankings.reverse()\n",
    "    # returns the recommended items\n",
    "    recommendataions_list = [recommend_item for score,recommend_item in rankings]\n",
    "    return recommendataions_list[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lawrence Toppman', 'Michael Wilmington', 'David Edelstein', 'Ty Burr', 'Desson Thomson', 'Peter Travers', 'Roger Ebert', 'James Berardinelli', 'Stephen Holden', 'Kenneth Turan', 'David Sterritt', 'Wesley Morris', 'Jonathan Rosenbaum', 'Manohla Dargis', 'Lisa Schwarzbaum', 'Michael Phillips', 'Roger Moore', 'Dana Stevens', 'Steven Rea', 'Joe Morgenstern', \"Michael O'Sullivan\", 'Rene Rodriguez', 'Michael Sragow', 'Todd McCarthy', 'Peter Rainer']\n",
      "Total number of critics in the dataset=25\n"
     ]
    }
   ],
   "source": [
    "print dataset.keys()\n",
    "print \"Total number of critics in the dataset=%d\" % len(dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1429\n"
     ]
    }
   ],
   "source": [
    "'''Total ratings given by the first critic '''\n",
    "print len(dataset.items()[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40950\n"
     ]
    }
   ],
   "source": [
    "'''Total number of movie ratings given by all critics'''\n",
    "c=0\n",
    "for i in dataset.keys():\n",
    "    c=c+len(dataset[i])\n",
    "print c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Who Framed Roger Rabbit', 'Tootsie', 'The Wizard of Oz (re-release)', 'Raging Bull', 'Patton', 'National Gallery', 'From Here to Eternity (re-release)', 'Frantic [re-release]', 'Beau Travail', 'Z Channel: A Magnificent Obsession', 'You, the Living', 'Where Are You Taking Me?', 'When Marnie Was There', 'What Richard Did', 'Werckmeister Harmonies', 'Waging a Living', 'Umberto D (re-release)', 'Two Women', 'Two Step', 'This Filthy World']\n"
     ]
    }
   ],
   "source": [
    "'''Get top 20 recommended movies for critic Michael Phillips by estimating \n",
    "    the correlation score betweem him and all other critics'''\n",
    "\n",
    "print user_reommendations('Michael Phillips',20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.6142933202957855, 'Ty Burr'), (0.607544288227396, 'Roger Moore'), (0.5851445957264291, 'Kenneth Turan')]\n"
     ]
    }
   ],
   "source": [
    "'''Print the most similar critics to critic Todd McCarthy'''\n",
    "print most_similar_users('Todd McCarthy',3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lawrence Toppman': 1429, 'Michael Wilmington': 1075, 'Desson Thomson': 1604, 'Ty Burr': 1692, 'David Edelstein': 1706, 'Dana Stevens': 1069, 'Roger Ebert': 2738, 'James Berardinelli': 2266, 'Stephen Holden': 1945, 'Kenneth Turan': 1887, 'David Sterritt': 1859, 'Wesley Morris': 1377, 'Jonathan Rosenbaum': 1416, 'Manohla Dargis': 1612, 'Lisa Schwarzbaum': 1781, 'Michael Phillips': 1381, 'Roger Moore': 1203, 'Peter Travers': 2221, 'Steven Rea': 1602, 'Joe Morgenstern': 1841, \"Michael O'Sullivan\": 1146, 'Rene Rodriguez': 1621, 'Michael Sragow': 1031, 'Todd McCarthy': 1365, 'Peter Rainer': 2083}\n"
     ]
    }
   ],
   "source": [
    "'''Calculate the length of ratings i.e, the total movies rated by each critic'''\n",
    "r={}\n",
    "for i in dataset.keys():\n",
    "    r[i] = len(dataset[i])\n",
    "print r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lawrence Toppman': 64.809657102869139, 'Michael Wilmington': 72.836279069767443, 'Desson Thomson': 59.748129675810475, 'Ty Burr': 65.086879432624116, 'David Edelstein': 64.167643610785461, 'Dana Stevens': 61.973807296538823, 'Roger Ebert': 68.58436815193572, 'James Berardinelli': 64.287290379523384, 'Stephen Holden': 58.750642673521853, 'Kenneth Turan': 70.752517223105457, 'David Sterritt': 66.463152232382996, 'Wesley Morris': 57.913580246913583, 'Jonathan Rosenbaum': 59.738700564971751, 'Manohla Dargis': 60.899503722084368, 'Lisa Schwarzbaum': 69.112296462661419, 'Michael Phillips': 65.485155684286752, 'Roger Moore': 59.088113050706568, 'Peter Travers': 65.621791985592083, 'Steven Rea': 70.595505617977523, 'Joe Morgenstern': 59.782726778924498, \"Michael O'Sullivan\": 58.139616055846425, 'Rene Rodriguez': 63.198025909932142, 'Michael Sragow': 64.676042677012603, 'Todd McCarthy': 62.065934065934066, 'Peter Rainer': 66.151704272683631}\n"
     ]
    }
   ],
   "source": [
    "'''Calculate average rating given by each critic'''\n",
    "\n",
    "import scipy\n",
    "avg={}\n",
    "for i in dataset.keys():\n",
    "    samp=[]\n",
    "    for v in dataset[i]:\n",
    "        samp.append(dataset[i][v])\n",
    "        a=scipy.mean(samp)\n",
    "        avg[i] = a\n",
    "print avg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
